{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "/Users/jessicagarson/anaconda/lib/python2.7/site-packages/pytz/__init__.py:29: UserWarning: Module argparse was already imported from /Users/jessicagarson/anaconda/python.app/Contents/lib/python2.7/argparse.pyc, but /Users/jessicagarson/anaconda/lib/python2.7/site-packages is being added to sys.path\n",
        "  from pkg_resources import resource_stream\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "train = pd.read_csv( \"labeledTrainData.tsv\", header=0, \n",
      " delimiter=\"\\t\", quoting=3 )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "train.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>id</th>\n",
        "      <th>sentiment</th>\n",
        "      <th>review</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>0</th>\n",
        "      <td> \"5814_8\"</td>\n",
        "      <td> 1</td>\n",
        "      <td> \"With all this stuff going down at the moment ...</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1</th>\n",
        "      <td> \"2381_9\"</td>\n",
        "      <td> 1</td>\n",
        "      <td> \"\\\"The Classic War of the Worlds\\\" by Timothy ...</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2</th>\n",
        "      <td> \"7759_3\"</td>\n",
        "      <td> 0</td>\n",
        "      <td> \"The film starts with a manager (Nicholas Bell...</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>3</th>\n",
        "      <td> \"3630_4\"</td>\n",
        "      <td> 0</td>\n",
        "      <td> \"It must be assumed that those who praised thi...</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>4</th>\n",
        "      <td> \"9495_8\"</td>\n",
        "      <td> 1</td>\n",
        "      <td> \"Superbly trashy and wondrously unpretentious ...</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 4,
       "text": [
        "         id  sentiment                                             review\n",
        "0  \"5814_8\"          1  \"With all this stuff going down at the moment ...\n",
        "1  \"2381_9\"          1  \"\\\"The Classic War of the Worlds\\\" by Timothy ...\n",
        "2  \"7759_3\"          0  \"The film starts with a manager (Nicholas Bell...\n",
        "3  \"3630_4\"          0  \"It must be assumed that those who praised thi...\n",
        "4  \"9495_8\"          1  \"Superbly trashy and wondrously unpretentious ..."
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "test = pd.read_csv( \"testData.tsv\", header=0, delimiter=\"\\t\", quoting=3)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "unlabeled_train = pd.read_csv( \"unlabeledTrainData.tsv\", header=0, \n",
      " delimiter=\"\\t\", quoting=3 )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "test.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>id</th>\n",
        "      <th>review</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>0</th>\n",
        "      <td> \"12311_10\"</td>\n",
        "      <td> \"Naturally in a film who's main themes are of ...</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1</th>\n",
        "      <td>   \"8348_2\"</td>\n",
        "      <td> \"This movie is a disaster within a disaster fi...</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2</th>\n",
        "      <td>   \"5828_4\"</td>\n",
        "      <td> \"All in all, this is a movie for kids. We saw ...</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>3</th>\n",
        "      <td>   \"7186_2\"</td>\n",
        "      <td> \"Afraid of the Dark left me with the impressio...</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>4</th>\n",
        "      <td>  \"12128_7\"</td>\n",
        "      <td> \"A very accurate depiction of small time mob l...</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 6,
       "text": [
        "           id                                             review\n",
        "0  \"12311_10\"  \"Naturally in a film who's main themes are of ...\n",
        "1    \"8348_2\"  \"This movie is a disaster within a disaster fi...\n",
        "2    \"5828_4\"  \"All in all, this is a movie for kids. We saw ...\n",
        "3    \"7186_2\"  \"Afraid of the Dark left me with the impressio...\n",
        "4   \"12128_7\"  \"A very accurate depiction of small time mob l..."
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print \"Read %d labeled train reviews, %d labeled test reviews, \" \\\n",
      " \"and %d unlabeled reviews\\n\" % (train[\"review\"].size,  \n",
      " test[\"review\"].size, unlabeled_train[\"review\"].size )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Read 25000 labeled train reviews, 25000 labeled test reviews, and 50000 unlabeled reviews\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from bs4 import BeautifulSoup\n",
      "import re\n",
      "from nltk.corpus import stopwords"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def review_to_wordlist( review, remove_stopwords=False ):\n",
      "    review_text = BeautifulSoup(review).get_text()\n",
      "    review_text = re.sub(\"[^a-zA-Z]\",\" \", review_text)\n",
      "    words = review_text.lower().split()\n",
      "    if remove_stopwords:\n",
      "        stops = set(stopwords.words(\"english\"))\n",
      "        words = [w for w in words if not w in stops]\n",
      "    return(words)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import nltk.data\n",
      "nltk.download()  "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "showing info http://nltk.github.com/nltk_data/\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 10,
       "text": [
        "True"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def review_to_sentences( review, tokenizer, remove_stopwords=False ):\n",
      "    raw_sentences = tokenizer.tokenize(review.strip())\n",
      "    sentences = []\n",
      "    for raw_sentence in raw_sentences:\n",
      "        if len(raw_sentence) > 0:\n",
      "            sentences.append( review_to_wordlist( raw_sentence, \\\n",
      "              remove_stopwords ))\n",
      "\n",
      "    return sentences"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sentences = []  \n",
      "\n",
      "print \"Parsing sentences from training set\"\n",
      "for review in train[\"review\"]:\n",
      "    sentences += review_to_sentences(review, tokenizer)\n",
      "\n",
      "print \"Parsing sentences from unlabeled set\"\n",
      "for review in unlabeled_train[\"review\"]:\n",
      "    sentences += review_to_sentences(review, tokenizer)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "/Users/jessicagarson/anaconda/lib/python2.7/site-packages/bs4/__init__.py:169: UserWarning: \".\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
        "  '\"%s\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.' % markup)\n",
        "/Users/jessicagarson/anaconda/lib/python2.7/site-packages/bs4/__init__.py:176: UserWarning: \"http://www.happierabroad.com\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.\n",
        "  '\"%s\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.' % markup)\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Parsing sentences from training set\n",
        "Parsing sentences from unlabeled set"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "/Users/jessicagarson/anaconda/lib/python2.7/site-packages/bs4/__init__.py:176: UserWarning: \"http://www.archive.org/details/LovefromaStranger\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.\n",
        "  '\"%s\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.' % markup)\n",
        "/Users/jessicagarson/anaconda/lib/python2.7/site-packages/bs4/__init__.py:176: UserWarning: \"http://www.loosechangeguide.com/LooseChangeGuide.html\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.\n",
        "  '\"%s\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.' % markup)\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "/Users/jessicagarson/anaconda/lib/python2.7/site-packages/bs4/__init__.py:176: UserWarning: \"http://www.msnbc.msn.com/id/4972055/site/newsweek/\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.\n",
        "  '\"%s\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.' % markup)\n",
        "/Users/jessicagarson/anaconda/lib/python2.7/site-packages/bs4/__init__.py:169: UserWarning: \"..\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
        "  '\"%s\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.' % markup)\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "/Users/jessicagarson/anaconda/lib/python2.7/site-packages/bs4/__init__.py:176: UserWarning: \"http://www.youtube.com/watch?v=a0KSqelmgN8\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.\n",
        "  '\"%s\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.' % markup)\n",
        "/Users/jessicagarson/anaconda/lib/python2.7/site-packages/bs4/__init__.py:176: UserWarning: \"http://jake-weird.blogspot.com/2007/08/beneath.html\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.\n",
        "  '\"%s\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.' % markup)\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print len(sentences)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "857234\n"
       ]
      }
     ],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print sentences[0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[u'with', u'all', u'this', u'stuff', u'going', u'down', u'at', u'the', u'moment', u'with', u'mj', u'i', u've', u'started', u'listening', u'to', u'his', u'music', u'watching', u'the', u'odd', u'documentary', u'here', u'and', u'there', u'watched', u'the', u'wiz', u'and', u'watched', u'moonwalker', u'again']\n"
       ]
      }
     ],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import logging"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 16
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',\\\n",
      "    level=logging.INFO)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "num_features = 300                       \n",
      "min_word_count = 40                      \n",
      "num_workers = 4      \n",
      "context = 10                                                                                      \n",
      "downsampling = 1e-3   "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Training model"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 21
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from gensim.models import word2vec"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2015-04-13 10:06:26,651 : INFO : Could not import Theano, will use standard float for default ShardedCorpus dtype.\n"
       ]
      }
     ],
     "prompt_number": 26
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "model = word2vec.Word2Vec(sentences, workers=num_workers, \\\n",
      "            size=num_features, min_count = min_word_count, \\\n",
      "            window = context, sample = downsampling)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2015-04-13 10:06:46,815 : INFO : collecting all words and their counts\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2015-04-13 10:06:46,816 : INFO : PROGRESS: at sentence #0, processed 0 words and 0 word types\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2015-04-13 10:06:47,095 : INFO : PROGRESS: at sentence #10000, processed 208855 words and 17012 word types\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2015-04-13 10:06:47,307 : INFO : PROGRESS: at sentence #20000, processed 418111 words and 24052 word types\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2015-04-13 10:06:47,501 : INFO : PROGRESS: at sentence #30000, processed 623000 words and 28990 word types\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2015-04-13 10:06:47,661 : INFO : PROGRESS: at sentence #40000, processed 833386 words and 33174 word types\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2015-04-13 10:06:47,818 : INFO : PROGRESS: at sentence #50000, processed 1042217 words and 36631 word types\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2015-04-13 10:06:47,990 : INFO : PROGRESS: at sentence #60000, processed 1246060 words and 39617 word types\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2015-04-13 10:06:51,204 : INFO : PROGRESS: at sentence #70000, processed 1449036 words and 42066 word types\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2015-04-13 10:06:51,377 : INFO : PROGRESS: at sentence #80000, processed 1656680 words and 44447 word types\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2015-04-13 10:06:51,549 : INFO : PROGRESS: at sentence #90000, processed 1862796 words and 46594 word types\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2015-04-13 10:06:51,749 : INFO : PROGRESS: at sentence #100000, processed 2066734 words and 48792 word types\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2015-04-13 10:06:51,939 : INFO : PROGRESS: at sentence #110000, processed 2271850 words and 50563 word types\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2015-04-13 10:06:52,100 : INFO : PROGRESS: at sentence #120000, processed 2478900 words and 52380 word types\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2015-04-13 10:06:52,255 : INFO : PROGRESS: at sentence #130000, processed 2685365 words and 54247 word types\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2015-04-13 10:06:52,398 : INFO : PROGRESS: at sentence #140000, processed 2894408 words and 55848 word types\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2015-04-13 10:06:52,540 : INFO : PROGRESS: at sentence #150000, processed 3092641 words and 57252 word types\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2015-04-13 10:06:52,721 : INFO : PROGRESS: at sentence #160000, processed 3302744 words and 58816 word types\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2015-04-13 10:06:52,908 : INFO : PROGRESS: at sentence #170000, processed 3509579 words and 60313 word types\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2015-04-13 10:06:53,061 : INFO : PROGRESS: at sentence #180000, processed 3714497 words and 61661 word types\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2015-04-13 10:06:53,209 : INFO : PROGRESS: at sentence #190000, processed 3919178 words and 62998 word types\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2015-04-13 10:06:53,355 : INFO : PROGRESS: at sentence #200000, processed 4127713 words and 64214 word types\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2015-04-13 10:06:53,485 : INFO : PROGRESS: at sentence #210000, processed 4334881 words and 65446 word types\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2015-04-13 10:06:53,611 : INFO : PROGRESS: at sentence #220000, processed 4540608 words and 66590 word types\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2015-04-13 10:06:53,751 : INFO : PROGRESS: at sentence #230000, processed 4749012 words and 67819 word types\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2015-04-13 10:06:53,909 : INFO : PROGRESS: at sentence #240000, processed 4958517 words and 69057 word types\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2015-04-13 10:06:54,041 : INFO : PROGRESS: at sentence #250000, processed 5166727 words and 70213 word types\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2015-04-13 10:06:54,179 : INFO : PROGRESS: at sentence #260000, processed 5374642 words and 71339 word types\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2015-04-13 10:06:54,314 : INFO : PROGRESS: at sentence #270000, processed 5570831 words and 72420 word types\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2015-04-13 10:06:54,455 : INFO : PROGRESS: at sentence #280000, processed 5778145 words and 73478 word types\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2015-04-13 10:06:54,588 : INFO : PROGRESS: at sentence #290000, processed 5981874 words and 74642 word types\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2015-04-13 10:06:54,725 : INFO : PROGRESS: at sentence #300000, processed 6191586 words and 76101 word types\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2015-04-13 10:06:54,852 : INFO : PROGRESS: at sentence #310000, processed 6400108 words and 77479 word types\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2015-04-13 10:06:54,981 : INFO : PROGRESS: at sentence #320000, processed 6608193 words and 78807 word types\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2015-04-13 10:06:55,117 : INFO : PROGRESS: at sentence #330000, processed 6816932 words and 80007 word types\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2015-04-13 10:06:55,248 : INFO : PROGRESS: at sentence #340000, processed 7022649 words and 81269 word types\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2015-04-13 10:06:55,381 : INFO : PROGRESS: at sentence #350000, processed 7233332 words and 82412 word types\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2015-04-13 10:06:55,514 : INFO : PROGRESS: at sentence #360000, processed 7441657 words and 83540 word types\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2015-04-13 10:06:55,644 : INFO : PROGRESS: at sentence #370000, processed 7651026 words and 84709 word types\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2015-04-13 10:06:55,842 : INFO : PROGRESS: at sentence #380000, processed 7857897 words and 85755 word types\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2015-04-13 10:06:55,969 : INFO : PROGRESS: at sentence #390000, processed 8064572 words and 86826 word types\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2015-04-13 10:06:56,117 : INFO : PROGRESS: at sentence #400000, processed 8275058 words and 87861 word types\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2015-04-13 10:06:56,292 : INFO : PROGRESS: at sentence #410000, processed 8483256 words and 88917 word types\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2015-04-13 10:06:56,489 : INFO : PROGRESS: at sentence #420000, processed 8695904 words and 89891 word types\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2015-04-13 10:06:56,687 : INFO : PROGRESS: at sentence #430000, processed 8904179 words and 90837 word types\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2015-04-13 10:06:56,874 : INFO : PROGRESS: at sentence #440000, processed 9106911 words and 91725 word types\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2015-04-13 10:06:57,080 : INFO : PROGRESS: at sentence #450000, processed 9316316 words and 92692 word types\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2015-04-13 10:06:57,287 : INFO : PROGRESS: at sentence #460000, processed 9522194 words and 93620 word types\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2015-04-13 10:06:57,480 : INFO : PROGRESS: at sentence #470000, processed 9734830 words and 94546 word types\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2015-04-13 10:06:57,669 : INFO : PROGRESS: at sentence #480000, processed 9940858 words and 95525 word types\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2015-04-13 10:06:57,862 : INFO : PROGRESS: at sentence #490000, processed 10152284 words and 96470 word types\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2015-04-13 10:06:58,066 : INFO : PROGRESS: at sentence #500000, processed 10367183 words and 97419 word types\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2015-04-13 10:06:58,264 : INFO : PROGRESS: at sentence #510000, processed 10578245 words and 98253 word types\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2015-04-13 10:06:58,446 : INFO : PROGRESS: at sentence #520000, processed 10784156 words and 99149 word types\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2015-04-13 10:06:58,639 : INFO : PROGRESS: at sentence #530000, processed 10993251 words and 100031 word types\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2015-04-13 10:06:58,801 : INFO : PROGRESS: at sentence #540000, processed 11199386 words and 100900 word types\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2015-04-13 10:06:58,932 : INFO : PROGRESS: at sentence #550000, processed 11408654 words and 101724 word types\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2015-04-13 10:06:59,060 : INFO : PROGRESS: at sentence #560000, processed 11615837 words and 102565 word types\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2015-04-13 10:06:59,190 : INFO : PROGRESS: at sentence #570000, processed 11826389 words and 103335 word types\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2015-04-13 10:06:59,342 : INFO : PROGRESS: at sentence #580000, processed 12032646 words and 104124 word types\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2015-04-13 10:06:59,480 : INFO : PROGRESS: at sentence #590000, processed 12238513 words and 104940 word types\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2015-04-13 10:06:59,619 : INFO : PROGRESS: at sentence #600000, processed 12448616 words and 105737 word types\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2015-04-13 10:06:59,754 : INFO : PROGRESS: at sentence #610000, processed 12658290 words and 106489 word types\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2015-04-13 10:06:59,933 : INFO : PROGRESS: at sentence #620000, processed 12868467 words and 107274 word types\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2015-04-13 10:07:00,083 : INFO : PROGRESS: at sentence #630000, processed 13076585 words and 108021 word types\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2015-04-13 10:07:00,211 : INFO : PROGRESS: at sentence #640000, processed 13282788 words and 108746 word types\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2015-04-13 10:07:00,361 : INFO : PROGRESS: at sentence #650000, processed 13487285 words and 109475 word types\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2015-04-13 10:07:00,615 : INFO : PROGRESS: at sentence #660000, processed 13698126 words and 110294 word types\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2015-04-13 10:07:00,877 : INFO : PROGRESS: at sentence #670000, processed 13906622 words and 110965 word types\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2015-04-13 10:07:01,096 : INFO : PROGRESS: at sentence #680000, processed 14111305 words and 111684 word types\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2015-04-13 10:07:01,265 : INFO : PROGRESS: at sentence #690000, processed 14317329 words and 112441 word types\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2015-04-13 10:07:01,421 : INFO : PROGRESS: at sentence #700000, processed 14527654 words and 113174 word types\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2015-04-13 10:07:01,553 : INFO : PROGRESS: at sentence #710000, processed 14734510 words and 113859 word types\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2015-04-13 10:07:01,686 : INFO : PROGRESS: at sentence #720000, processed 14940975 words and 114511 word types\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2015-04-13 10:07:01,822 : INFO : PROGRESS: at sentence #730000, processed 15152207 words and 115206 word types\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2015-04-13 10:07:01,954 : INFO : PROGRESS: at sentence #740000, processed 15356467 words and 115844 word types\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2015-04-13 10:07:02,088 : INFO : PROGRESS: at sentence #750000, processed 15564190 words and 116584 word types\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2015-04-13 10:07:02,228 : INFO : PROGRESS: at sentence #760000, processed 15776656 words and 117267 word types\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2015-04-13 10:07:02,361 : INFO : PROGRESS: at sentence #770000, processed 15986378 words and 117888 word types\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2015-04-13 10:07:02,485 : INFO : PROGRESS: at sentence #780000, processed 16191009 words and 118535 word types\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2015-04-13 10:07:02,612 : INFO : PROGRESS: at sentence #790000, processed 16400140 words and 119167 word types\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2015-04-13 10:07:02,738 : INFO : PROGRESS: at sentence #800000, processed 16606548 words and 119818 word types\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2015-04-13 10:07:02,918 : INFO : PROGRESS: at sentence #810000, processed 16809994 words and 120439 word types\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2015-04-13 10:07:03,072 : INFO : PROGRESS: at sentence #820000, processed 17010701 words and 121037 word types\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2015-04-13 10:07:03,209 : INFO : PROGRESS: at sentence #830000, processed 17223507 words and 121721 word types\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2015-04-13 10:07:03,412 : INFO : PROGRESS: at sentence #840000, processed 17438171 words and 122371 word types\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2015-04-13 10:07:03,595 : INFO : PROGRESS: at sentence #850000, processed 17647157 words and 122995 word types\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2015-04-13 10:07:03,772 : INFO : collected 123504 word types from a corpus of 17797876 words and 857234 sentences\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2015-04-13 10:07:03,905 : INFO : total 16490 word types after removing those with count<40\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2015-04-13 10:07:03,906 : INFO : constructing a huffman tree from 16490 words\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2015-04-13 10:07:04,789 : INFO : built huffman tree with maximum node depth 19\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2015-04-13 10:07:04,799 : INFO : frequent-word downsampling, threshold 0.001; progress tallies will be approximate\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2015-04-13 10:07:04,879 : INFO : resetting layer weights\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2015-04-13 10:07:05,310 : INFO : training model with 4 workers on 16490 vocabulary and 300 features, using 'skipgram'=1 'hierarchical softmax'=1 'subsample'=0.001 and 'negative sampling'=0\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2015-04-13 10:07:06,324 : INFO : PROGRESS: at 0.54% words, alpha 0.02487, 68011 words/s\n",
        "2015-04-13 10:07:07,338 : INFO : PROGRESS: at 1.11% words, alpha 0.02473, 70132 words/s\n",
        "2015-04-13 10:07:08,346 : INFO : PROGRESS: at 1.63% words, alpha 0.02460, 68529 words/s\n",
        "2015-04-13 10:07:09,350 : INFO : PROGRESS: at 2.12% words, alpha 0.02448, 66798 words/s\n",
        "2015-04-13 10:07:10,377 : INFO : PROGRESS: at 2.61% words, alpha 0.02436, 65742 words/s\n",
        "2015-04-13 10:07:11,392 : INFO : PROGRESS: at 3.16% words, alpha 0.02422, 66262 words/s\n",
        "2015-04-13 10:07:12,431 : INFO : PROGRESS: at 3.70% words, alpha 0.02409, 66327 words/s\n",
        "2015-04-13 10:07:13,473 : INFO : PROGRESS: at 4.19% words, alpha 0.02397, 65453 words/s\n",
        "2015-04-13 10:07:14,476 : INFO : PROGRESS: at 4.61% words, alpha 0.02386, 64145 words/s\n",
        "2015-04-13 10:07:15,484 : INFO : PROGRESS: at 4.94% words, alpha 0.02377, 61899 words/s\n",
        "2015-04-13 10:07:16,520 : INFO : PROGRESS: at 5.44% words, alpha 0.02365, 61827 words/s\n",
        "2015-04-13 10:07:17,530 : INFO : PROGRESS: at 5.98% words, alpha 0.02352, 62374 words/s\n",
        "2015-04-13 10:07:18,537 : INFO : PROGRESS: at 6.48% words, alpha 0.02339, 62459 words/s\n",
        "2015-04-13 10:07:19,550 : INFO : PROGRESS: at 7.01% words, alpha 0.02326, 62779 words/s\n",
        "2015-04-13 10:07:20,571 : INFO : PROGRESS: at 7.59% words, alpha 0.02311, 63404 words/s\n",
        "2015-04-13 10:07:21,588 : INFO : PROGRESS: at 8.18% words, alpha 0.02296, 64101 words/s\n",
        "2015-04-13 10:07:22,599 : INFO : PROGRESS: at 8.69% words, alpha 0.02284, 64074 words/s\n",
        "2015-04-13 10:07:23,616 : INFO : PROGRESS: at 9.18% words, alpha 0.02272, 63918 words/s\n",
        "2015-04-13 10:07:24,670 : INFO : PROGRESS: at 9.73% words, alpha 0.02258, 64076 words/s\n",
        "2015-04-13 10:07:25,675 : INFO : PROGRESS: at 10.24% words, alpha 0.02246, 64133 words/s\n",
        "2015-04-13 10:07:26,713 : INFO : PROGRESS: at 10.82% words, alpha 0.02231, 64477 words/s\n",
        "2015-04-13 10:07:27,753 : INFO : PROGRESS: at 11.42% words, alpha 0.02215, 64883 words/s\n",
        "2015-04-13 10:07:28,789 : INFO : PROGRESS: at 12.02% words, alpha 0.02201, 65300 words/s\n",
        "2015-04-13 10:07:29,792 : INFO : PROGRESS: at 12.61% words, alpha 0.02186, 65658 words/s\n",
        "2015-04-13 10:07:30,800 : INFO : PROGRESS: at 13.16% words, alpha 0.02172, 65834 words/s\n",
        "2015-04-13 10:07:31,810 : INFO : PROGRESS: at 13.77% words, alpha 0.02158, 66247 words/s\n",
        "2015-04-13 10:07:32,815 : INFO : PROGRESS: at 14.35% words, alpha 0.02142, 66497 words/s\n",
        "2015-04-13 10:07:33,815 : INFO : PROGRESS: at 14.90% words, alpha 0.02129, 66667 words/s\n",
        "2015-04-13 10:07:34,829 : INFO : PROGRESS: at 15.47% words, alpha 0.02114, 66827 words/s\n",
        "2015-04-13 10:07:35,831 : INFO : PROGRESS: at 16.08% words, alpha 0.02099, 67159 words/s\n",
        "2015-04-13 10:07:36,856 : INFO : PROGRESS: at 16.65% words, alpha 0.02085, 67307 words/s\n",
        "2015-04-13 10:07:37,856 : INFO : PROGRESS: at 17.22% words, alpha 0.02070, 67472 words/s\n",
        "2015-04-13 10:07:38,868 : INFO : PROGRESS: at 17.80% words, alpha 0.02056, 67611 words/s\n",
        "2015-04-13 10:07:39,901 : INFO : PROGRESS: at 18.37% words, alpha 0.02042, 67713 words/s\n",
        "2015-04-13 10:07:40,935 : INFO : PROGRESS: at 18.98% words, alpha 0.02026, 67926 words/s\n",
        "2015-04-13 10:07:41,936 : INFO : PROGRESS: at 19.54% words, alpha 0.02012, 68013 words/s\n",
        "2015-04-13 10:07:42,939 : INFO : PROGRESS: at 20.13% words, alpha 0.01998, 68209 words/s\n",
        "2015-04-13 10:07:43,948 : INFO : PROGRESS: at 20.71% words, alpha 0.01983, 68327 words/s\n",
        "2015-04-13 10:07:44,976 : INFO : PROGRESS: at 21.28% words, alpha 0.01969, 68392 words/s\n",
        "2015-04-13 10:07:46,024 : INFO : PROGRESS: at 21.86% words, alpha 0.01954, 68447 words/s\n",
        "2015-04-13 10:07:47,032 : INFO : PROGRESS: at 22.28% words, alpha 0.01944, 68079 words/s\n",
        "2015-04-13 10:07:48,041 : INFO : PROGRESS: at 22.69% words, alpha 0.01934, 67711 words/s\n",
        "2015-04-13 10:07:49,075 : INFO : PROGRESS: at 23.21% words, alpha 0.01921, 67614 words/s\n",
        "2015-04-13 10:07:50,081 : INFO : PROGRESS: at 23.73% words, alpha 0.01908, 67575 words/s\n",
        "2015-04-13 10:07:51,100 : INFO : PROGRESS: at 24.22% words, alpha 0.01895, 67450 words/s\n",
        "2015-04-13 10:07:52,123 : INFO : PROGRESS: at 24.71% words, alpha 0.01883, 67289 words/s\n",
        "2015-04-13 10:07:53,132 : INFO : PROGRESS: at 25.25% words, alpha 0.01870, 67322 words/s\n",
        "2015-04-13 10:07:54,169 : INFO : PROGRESS: at 25.85% words, alpha 0.01855, 67449 words/s\n",
        "2015-04-13 10:07:55,176 : INFO : PROGRESS: at 26.43% words, alpha 0.01841, 67572 words/s\n",
        "2015-04-13 10:07:56,186 : INFO : PROGRESS: at 27.00% words, alpha 0.01826, 67658 words/s\n",
        "2015-04-13 10:07:57,191 : INFO : PROGRESS: at 27.61% words, alpha 0.01811, 67854 words/s\n",
        "2015-04-13 10:07:58,224 : INFO : PROGRESS: at 28.23% words, alpha 0.01795, 68016 words/s\n",
        "2015-04-13 10:07:59,253 : INFO : PROGRESS: at 28.83% words, alpha 0.01781, 68131 words/s\n",
        "2015-04-13 10:08:00,285 : INFO : PROGRESS: at 29.39% words, alpha 0.01766, 68160 words/s\n",
        "2015-04-13 10:08:01,327 : INFO : PROGRESS: at 29.98% words, alpha 0.01752, 68237 words/s\n",
        "2015-04-13 10:08:02,328 : INFO : PROGRESS: at 30.56% words, alpha 0.01737, 68329 words/s\n",
        "2015-04-13 10:08:03,345 : INFO : PROGRESS: at 31.13% words, alpha 0.01723, 68388 words/s\n",
        "2015-04-13 10:08:04,354 : INFO : PROGRESS: at 31.71% words, alpha 0.01708, 68478 words/s\n",
        "2015-04-13 10:08:05,360 : INFO : PROGRESS: at 32.29% words, alpha 0.01694, 68561 words/s\n",
        "2015-04-13 10:08:06,393 : INFO : PROGRESS: at 32.87% words, alpha 0.01679, 68599 words/s\n",
        "2015-04-13 10:08:07,398 : INFO : PROGRESS: at 33.43% words, alpha 0.01665, 68646 words/s\n",
        "2015-04-13 10:08:08,398 : INFO : PROGRESS: at 34.00% words, alpha 0.01651, 68717 words/s\n",
        "2015-04-13 10:08:09,422 : INFO : PROGRESS: at 34.55% words, alpha 0.01637, 68712 words/s\n",
        "2015-04-13 10:08:10,438 : INFO : PROGRESS: at 34.98% words, alpha 0.01626, 68486 words/s\n",
        "2015-04-13 10:08:11,442 : INFO : PROGRESS: at 35.55% words, alpha 0.01612, 68543 words/s\n",
        "2015-04-13 10:08:12,463 : INFO : PROGRESS: at 36.03% words, alpha 0.01601, 68400 words/s\n",
        "2015-04-13 10:08:13,467 : INFO : PROGRESS: at 36.50% words, alpha 0.01588, 68273 words/s\n",
        "2015-04-13 10:08:14,476 : INFO : PROGRESS: at 37.05% words, alpha 0.01575, 68288 words/s\n",
        "2015-04-13 10:08:15,484 : INFO : PROGRESS: at 37.59% words, alpha 0.01562, 68301 words/s\n",
        "2015-04-13 10:08:16,494 : INFO : PROGRESS: at 38.19% words, alpha 0.01546, 68410 words/s\n",
        "2015-04-13 10:08:17,535 : INFO : PROGRESS: at 38.81% words, alpha 0.01531, 68512 words/s\n",
        "2015-04-13 10:08:18,558 : INFO : PROGRESS: at 39.18% words, alpha 0.01522, 68202 words/s\n",
        "2015-04-13 10:08:19,574 : INFO : PROGRESS: at 39.56% words, alpha 0.01512, 67915 words/s\n",
        "2015-04-13 10:08:20,583 : INFO : PROGRESS: at 40.09% words, alpha 0.01500, 67902 words/s\n",
        "2015-04-13 10:08:21,590 : INFO : PROGRESS: at 40.62% words, alpha 0.01486, 67891 words/s\n",
        "2015-04-13 10:08:22,601 : INFO : PROGRESS: at 41.16% words, alpha 0.01472, 67891 words/s\n",
        "2015-04-13 10:08:23,607 : INFO : PROGRESS: at 41.72% words, alpha 0.01458, 67939 words/s\n",
        "2015-04-13 10:08:24,662 : INFO : PROGRESS: at 42.34% words, alpha 0.01443, 68023 words/s\n",
        "2015-04-13 10:08:25,679 : INFO : PROGRESS: at 42.94% words, alpha 0.01428, 68125 words/s\n",
        "2015-04-13 10:08:26,687 : INFO : PROGRESS: at 43.52% words, alpha 0.01413, 68186 words/s\n",
        "2015-04-13 10:08:27,691 : INFO : PROGRESS: at 44.07% words, alpha 0.01399, 68208 words/s\n",
        "2015-04-13 10:08:28,720 : INFO : PROGRESS: at 44.65% words, alpha 0.01385, 68257 words/s\n",
        "2015-04-13 10:08:29,724 : INFO : PROGRESS: at 45.23% words, alpha 0.01370, 68316 words/s\n",
        "2015-04-13 10:08:30,737 : INFO : PROGRESS: at 45.81% words, alpha 0.01356, 68369 words/s\n",
        "2015-04-13 10:08:31,739 : INFO : PROGRESS: at 46.38% words, alpha 0.01342, 68415 words/s\n",
        "2015-04-13 10:08:32,769 : INFO : PROGRESS: at 46.97% words, alpha 0.01327, 68469 words/s\n",
        "2015-04-13 10:08:33,786 : INFO : PROGRESS: at 47.55% words, alpha 0.01313, 68516 words/s\n",
        "2015-04-13 10:08:34,788 : INFO : PROGRESS: at 48.14% words, alpha 0.01298, 68596 words/s\n",
        "2015-04-13 10:08:35,796 : INFO : PROGRESS: at 48.73% words, alpha 0.01283, 68658 words/s\n",
        "2015-04-13 10:08:36,810 : INFO : PROGRESS: at 49.29% words, alpha 0.01269, 68676 words/s\n",
        "2015-04-13 10:08:37,810 : INFO : PROGRESS: at 49.81% words, alpha 0.01256, 68656 words/s\n",
        "2015-04-13 10:08:38,821 : INFO : PROGRESS: at 50.39% words, alpha 0.01241, 68704 words/s\n",
        "2015-04-13 10:08:39,832 : INFO : PROGRESS: at 50.99% words, alpha 0.01226, 68772 words/s\n",
        "2015-04-13 10:08:40,852 : INFO : PROGRESS: at 51.59% words, alpha 0.01211, 68847 words/s\n",
        "2015-04-13 10:08:41,889 : INFO : PROGRESS: at 52.18% words, alpha 0.01196, 68883 words/s\n",
        "2015-04-13 10:08:42,895 : INFO : PROGRESS: at 52.78% words, alpha 0.01181, 68962 words/s\n",
        "2015-04-13 10:08:43,900 : INFO : PROGRESS: at 53.38% words, alpha 0.01166, 69037 words/s\n",
        "2015-04-13 10:08:44,916 : INFO : PROGRESS: at 53.99% words, alpha 0.01151, 69109 words/s\n",
        "2015-04-13 10:08:45,927 : INFO : PROGRESS: at 54.60% words, alpha 0.01136, 69182 words/s\n",
        "2015-04-13 10:08:46,936 : INFO : PROGRESS: at 55.19% words, alpha 0.01121, 69233 words/s\n",
        "2015-04-13 10:08:47,953 : INFO : PROGRESS: at 55.80% words, alpha 0.01106, 69309 words/s\n",
        "2015-04-13 10:08:48,966 : INFO : PROGRESS: at 56.42% words, alpha 0.01091, 69402 words/s\n",
        "2015-04-13 10:08:49,975 : INFO : PROGRESS: at 57.00% words, alpha 0.01076, 69440 words/s\n",
        "2015-04-13 10:08:50,992 : INFO : PROGRESS: at 57.59% words, alpha 0.01062, 69476 words/s\n",
        "2015-04-13 10:08:51,998 : INFO : PROGRESS: at 58.20% words, alpha 0.01046, 69554 words/s\n",
        "2015-04-13 10:08:52,999 : INFO : PROGRESS: at 58.78% words, alpha 0.01031, 69590 words/s\n",
        "2015-04-13 10:08:54,009 : INFO : PROGRESS: at 59.38% words, alpha 0.01017, 69644 words/s\n",
        "2015-04-13 10:08:55,042 : INFO : PROGRESS: at 60.01% words, alpha 0.01000, 69725 words/s\n",
        "2015-04-13 10:08:56,046 : INFO : PROGRESS: at 60.61% words, alpha 0.00986, 69781 words/s\n",
        "2015-04-13 10:08:57,068 : INFO : PROGRESS: at 61.22% words, alpha 0.00971, 69841 words/s\n",
        "2015-04-13 10:08:58,072 : INFO : PROGRESS: at 61.84% words, alpha 0.00955, 69920 words/s\n",
        "2015-04-13 10:08:59,073 : INFO : PROGRESS: at 62.43% words, alpha 0.00940, 69964 words/s\n",
        "2015-04-13 10:09:00,087 : INFO : PROGRESS: at 63.02% words, alpha 0.00926, 70005 words/s\n",
        "2015-04-13 10:09:01,090 : INFO : PROGRESS: at 63.62% words, alpha 0.00910, 70060 words/s\n",
        "2015-04-13 10:09:02,092 : INFO : PROGRESS: at 64.17% words, alpha 0.00897, 70060 words/s\n",
        "2015-04-13 10:09:03,132 : INFO : PROGRESS: at 64.70% words, alpha 0.00883, 70018 words/s\n",
        "2015-04-13 10:09:04,148 : INFO : PROGRESS: at 65.25% words, alpha 0.00870, 70009 words/s\n",
        "2015-04-13 10:09:05,175 : INFO : PROGRESS: at 65.86% words, alpha 0.00855, 70051 words/s\n",
        "2015-04-13 10:09:06,191 : INFO : PROGRESS: at 66.47% words, alpha 0.00840, 70109 words/s\n",
        "2015-04-13 10:09:07,213 : INFO : PROGRESS: at 67.06% words, alpha 0.00825, 70133 words/s\n",
        "2015-04-13 10:09:08,250 : INFO : PROGRESS: at 67.65% words, alpha 0.00809, 70161 words/s\n",
        "2015-04-13 10:09:09,259 : INFO : PROGRESS: at 68.28% words, alpha 0.00795, 70231 words/s\n",
        "2015-04-13 10:09:10,273 : INFO : PROGRESS: at 68.89% words, alpha 0.00780, 70289 words/s\n",
        "2015-04-13 10:09:11,309 : INFO : PROGRESS: at 69.52% words, alpha 0.00762, 70345 words/s\n",
        "2015-04-13 10:09:12,316 : INFO : PROGRESS: at 70.10% words, alpha 0.00749, 70369 words/s\n",
        "2015-04-13 10:09:13,332 : INFO : PROGRESS: at 70.69% words, alpha 0.00734, 70403 words/s\n",
        "2015-04-13 10:09:14,345 : INFO : PROGRESS: at 71.29% words, alpha 0.00718, 70440 words/s\n",
        "2015-04-13 10:09:15,356 : INFO : PROGRESS: at 71.88% words, alpha 0.00704, 70469 words/s\n",
        "2015-04-13 10:09:16,371 : INFO : PROGRESS: at 72.49% words, alpha 0.00688, 70517 words/s\n",
        "2015-04-13 10:09:17,375 : INFO : PROGRESS: at 73.08% words, alpha 0.00674, 70553 words/s\n",
        "2015-04-13 10:09:18,389 : INFO : PROGRESS: at 73.66% words, alpha 0.00660, 70572 words/s\n",
        "2015-04-13 10:09:19,398 : INFO : PROGRESS: at 74.27% words, alpha 0.00644, 70624 words/s\n",
        "2015-04-13 10:09:20,403 : INFO : PROGRESS: at 74.87% words, alpha 0.00629, 70659 words/s\n",
        "2015-04-13 10:09:21,409 : INFO : PROGRESS: at 75.44% words, alpha 0.00615, 70676 words/s\n",
        "2015-04-13 10:09:22,421 : INFO : PROGRESS: at 76.04% words, alpha 0.00600, 70709 words/s\n",
        "2015-04-13 10:09:23,438 : INFO : PROGRESS: at 76.65% words, alpha 0.00585, 70753 words/s\n",
        "2015-04-13 10:09:24,452 : INFO : PROGRESS: at 77.24% words, alpha 0.00570, 70773 words/s\n",
        "2015-04-13 10:09:25,455 : INFO : PROGRESS: at 77.81% words, alpha 0.00556, 70791 words/s\n",
        "2015-04-13 10:09:26,458 : INFO : PROGRESS: at 78.44% words, alpha 0.00541, 70850 words/s\n",
        "2015-04-13 10:09:27,497 : INFO : PROGRESS: at 79.02% words, alpha 0.00526, 70854 words/s\n",
        "2015-04-13 10:09:28,499 : INFO : PROGRESS: at 79.59% words, alpha 0.00511, 70868 words/s\n",
        "2015-04-13 10:09:29,518 : INFO : PROGRESS: at 80.20% words, alpha 0.00496, 70905 words/s\n",
        "2015-04-13 10:09:30,530 : INFO : PROGRESS: at 80.81% words, alpha 0.00481, 70949 words/s\n",
        "2015-04-13 10:09:31,533 : INFO : PROGRESS: at 81.43% words, alpha 0.00465, 70999 words/s\n",
        "2015-04-13 10:09:32,533 : INFO : PROGRESS: at 82.03% words, alpha 0.00451, 71040 words/s\n",
        "2015-04-13 10:09:33,535 : INFO : PROGRESS: at 82.63% words, alpha 0.00435, 71078 words/s\n",
        "2015-04-13 10:09:34,544 : INFO : PROGRESS: at 83.25% words, alpha 0.00420, 71121 words/s\n",
        "2015-04-13 10:09:35,549 : INFO : PROGRESS: at 83.85% words, alpha 0.00405, 71160 words/s\n",
        "2015-04-13 10:09:36,551 : INFO : PROGRESS: at 84.43% words, alpha 0.00390, 71173 words/s\n",
        "2015-04-13 10:09:37,567 : INFO : PROGRESS: at 85.00% words, alpha 0.00376, 71179 words/s\n",
        "2015-04-13 10:09:38,587 : INFO : PROGRESS: at 85.60% words, alpha 0.00362, 71201 words/s\n",
        "2015-04-13 10:09:39,614 : INFO : PROGRESS: at 86.19% words, alpha 0.00346, 71217 words/s\n",
        "2015-04-13 10:09:40,615 : INFO : PROGRESS: at 86.75% words, alpha 0.00333, 71213 words/s\n",
        "2015-04-13 10:09:41,636 : INFO : PROGRESS: at 87.34% words, alpha 0.00317, 71230 words/s\n",
        "2015-04-13 10:09:42,651 : INFO : PROGRESS: at 87.93% words, alpha 0.00303, 71253 words/s\n",
        "2015-04-13 10:09:43,658 : INFO : PROGRESS: at 88.53% words, alpha 0.00288, 71279 words/s\n",
        "2015-04-13 10:09:44,658 : INFO : PROGRESS: at 89.09% words, alpha 0.00274, 71284 words/s\n",
        "2015-04-13 10:09:45,659 : INFO : PROGRESS: at 89.67% words, alpha 0.00259, 71300 words/s\n",
        "2015-04-13 10:09:46,680 : INFO : PROGRESS: at 90.25% words, alpha 0.00244, 71308 words/s\n",
        "2015-04-13 10:09:47,698 : INFO : PROGRESS: at 90.86% words, alpha 0.00230, 71335 words/s\n",
        "2015-04-13 10:09:48,707 : INFO : PROGRESS: at 91.46% words, alpha 0.00215, 71366 words/s\n",
        "2015-04-13 10:09:49,716 : INFO : PROGRESS: at 92.09% words, alpha 0.00199, 71412 words/s\n",
        "2015-04-13 10:09:50,731 : INFO : PROGRESS: at 92.70% words, alpha 0.00184, 71448 words/s\n",
        "2015-04-13 10:09:51,762 : INFO : PROGRESS: at 93.31% words, alpha 0.00168, 71472 words/s\n",
        "2015-04-13 10:09:52,766 : INFO : PROGRESS: at 93.89% words, alpha 0.00154, 71482 words/s\n",
        "2015-04-13 10:09:53,771 : INFO : PROGRESS: at 94.47% words, alpha 0.00140, 71495 words/s\n",
        "2015-04-13 10:09:54,799 : INFO : PROGRESS: at 95.04% words, alpha 0.00125, 71496 words/s\n",
        "2015-04-13 10:09:55,808 : INFO : PROGRESS: at 95.64% words, alpha 0.00110, 71515 words/s\n",
        "2015-04-13 10:09:56,826 : INFO : PROGRESS: at 96.24% words, alpha 0.00095, 71543 words/s\n",
        "2015-04-13 10:09:57,851 : INFO : PROGRESS: at 96.83% words, alpha 0.00080, 71551 words/s\n",
        "2015-04-13 10:09:58,875 : INFO : PROGRESS: at 97.39% words, alpha 0.00067, 71538 words/s\n",
        "2015-04-13 10:09:59,876 : INFO : PROGRESS: at 97.94% words, alpha 0.00053, 71534 words/s\n",
        "2015-04-13 10:10:00,880 : INFO : PROGRESS: at 98.52% words, alpha 0.00038, 71543 words/s\n",
        "2015-04-13 10:10:01,912 : INFO : PROGRESS: at 99.16% words, alpha 0.00022, 71586 words/s\n",
        "2015-04-13 10:10:02,940 : INFO : PROGRESS: at 99.80% words, alpha 0.00010, 71631 words/s\n",
        "2015-04-13 10:10:03,030 : INFO : reached the end of input; waiting to finish 8 outstanding jobs\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2015-04-13 10:10:03,206 : INFO : training on 12748484 words took 177.9s, 71663 words/s\n"
       ]
      }
     ],
     "prompt_number": 27
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "model.init_sims(replace=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2015-04-13 10:51:59,514 : INFO : precomputing L2-norms of word weight vectors\n"
       ]
      }
     ],
     "prompt_number": 28
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "model_name = \"300features_40minwords_10context\"\n",
      "model.save(model_name)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2015-04-13 10:52:27,388 : INFO : saving Word2Vec object under 300features_40minwords_10context, separately None\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2015-04-13 10:52:27,389 : INFO : not storing attribute syn0norm\n"
       ]
      }
     ],
     "prompt_number": 29
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#checking cpu in terminal by running top -o cpu "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 31
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "model.doesnt_match(\"man woman child kitchen\".split())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 33,
       "text": [
        "'kitchen'"
       ]
      }
     ],
     "prompt_number": 33
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "model.doesnt_match(\"france england germany berlin\".split())\n",
      "'berlin'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 34,
       "text": [
        "'berlin'"
       ]
      }
     ],
     "prompt_number": 34
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "model.doesnt_match(\"paris berlin london austria\".split())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 35,
       "text": [
        "'paris'"
       ]
      }
     ],
     "prompt_number": 35
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      " model.most_similar(\"man\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 36,
       "text": [
        "[(u'woman', 0.627252995967865),\n",
        " (u'boy', 0.49006545543670654),\n",
        " (u'person', 0.4841436445713043),\n",
        " (u'lady', 0.4633031189441681),\n",
        " (u'guy', 0.45725637674331665),\n",
        " (u'men', 0.4450477361679077),\n",
        " (u'girl', 0.44117048382759094),\n",
        " (u'himself', 0.40582048892974854),\n",
        " (u'son', 0.39881205558776855),\n",
        " (u'doctor', 0.3900986313819885)]"
       ]
      }
     ],
     "prompt_number": 36
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "model.most_similar(\"queen\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 37,
       "text": [
        "[(u'princess', 0.4871219992637634),\n",
        " (u'latifah', 0.4544677734375),\n",
        " (u'king', 0.43055301904678345),\n",
        " (u'bee', 0.42681577801704407),\n",
        " (u'victoria', 0.4131453335285187),\n",
        " (u'mary', 0.37802526354789734),\n",
        " (u'prince', 0.37178754806518555),\n",
        " (u'lovely', 0.36760175228118896),\n",
        " (u'lion', 0.3672691583633423),\n",
        " (u'anne', 0.36102294921875)]"
       ]
      }
     ],
     "prompt_number": 37
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "model.most_similar(\"awful\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 38,
       "text": [
        "[(u'terrible', 0.6507499814033508),\n",
        " (u'horrible', 0.6265886425971985),\n",
        " (u'dreadful', 0.5902438163757324),\n",
        " (u'atrocious', 0.5747588276863098),\n",
        " (u'horrendous', 0.5452957153320312),\n",
        " (u'abysmal', 0.5371739864349365),\n",
        " (u'laughable', 0.5205896496772766),\n",
        " (u'appalling', 0.5103689432144165),\n",
        " (u'embarrassing', 0.5092446804046631),\n",
        " (u'amateurish', 0.5065664649009705)]"
       ]
      }
     ],
     "prompt_number": 38
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "type(model.syn0)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 39,
       "text": [
        "numpy.ndarray"
       ]
      }
     ],
     "prompt_number": 39
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "model.syn0.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 40,
       "text": [
        "(16490, 300)"
       ]
      }
     ],
     "prompt_number": 40
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 41
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np  # Make sure that numpy is imported\n",
      "\n",
      "def makeFeatureVec(words, model, num_features):\n",
      "    # Function to average all of the word vectors in a given\n",
      "    # paragraph\n",
      "    #\n",
      "    # Pre-initialize an empty numpy array (for speed)\n",
      "    featureVec = np.zeros((num_features,),dtype=\"float32\")\n",
      "    #\n",
      "    nwords = 0.\n",
      "    # \n",
      "    # Index2word is a list that contains the names of the words in \n",
      "    # the model's vocabulary. Convert it to a set, for speed \n",
      "    index2word_set = set(model.index2word)\n",
      "    #\n",
      "    # Loop over each word in the review and, if it is in the model's\n",
      "    # vocaublary, add its feature vector to the total\n",
      "    for word in words:\n",
      "        if word in index2word_set: \n",
      "            nwords = nwords + 1.\n",
      "            featureVec = np.add(featureVec,model[word])\n",
      "    # \n",
      "    # Divide the result by the number of words to get the average\n",
      "    featureVec = np.divide(featureVec,nwords)\n",
      "    return featureVec\n",
      "\n",
      "\n",
      "def getAvgFeatureVecs(reviews, model, num_features):\n",
      "    # Given a set of reviews (each one a list of words), calculate \n",
      "    # the average feature vector for each one and return a 2D numpy array \n",
      "    # \n",
      "    # Initialize a counter\n",
      "    counter = 0.\n",
      "    # \n",
      "    # Preallocate a 2D numpy array, for speed\n",
      "    reviewFeatureVecs = np.zeros((len(reviews),num_features),dtype=\"float32\")\n",
      "    # \n",
      "    # Loop through the reviews\n",
      "    for review in reviews:\n",
      "       #\n",
      "       # Print a status message every 1000th review\n",
      "       if counter%1000. == 0.:\n",
      "           print \"Review %d of %d\" % (counter, len(reviews))\n",
      "       # \n",
      "       # Call the function (defined above) that makes average feature vectors\n",
      "       reviewFeatureVecs[counter] = makeFeatureVec(review, model, \\\n",
      "           num_features)\n",
      "       #\n",
      "       # Increment the counter\n",
      "       counter = counter + 1.\n",
      "    return reviewFeatureVecs"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 45
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# ****************************************************************\n",
      "# Calculate average feature vectors for training and testing sets,\n",
      "# using the functions we defined above. Notice that we now use stop word\n",
      "# removal.\n",
      "\n",
      "clean_train_reviews = []\n",
      "for review in train[\"review\"]:\n",
      "    clean_train_reviews.append( review_to_wordlist( review, \\\n",
      "        remove_stopwords=True ))\n",
      "\n",
      "trainDataVecs = getAvgFeatureVecs( clean_train_reviews, model, num_features )\n",
      "\n",
      "print \"Creating average feature vecs for test reviews\"\n",
      "clean_test_reviews = []\n",
      "for review in test[\"review\"]:\n",
      "    clean_test_reviews.append( review_to_wordlist( review, \\\n",
      "        remove_stopwords=True ))\n",
      "\n",
      "testDataVecs = getAvgFeatureVecs( clean_test_reviews, model, num_features )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Review 0 of 25000\n",
        "Review 1000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 2000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 3000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 4000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 5000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 6000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 7000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 8000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 9000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 10000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 11000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 12000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 13000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 14000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 15000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 16000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 17000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 18000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 19000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 20000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 21000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 22000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 23000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 24000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Creating average feature vecs for test reviews"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 0 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 1000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 2000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 3000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 4000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 5000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 6000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 7000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 8000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 9000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 10000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 11000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 12000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 13000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 14000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 15000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 16000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 17000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 18000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 19000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 20000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 21000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 22000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 23000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 24000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 46
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.ensemble import RandomForestClassifier\n",
      "forest = RandomForestClassifier( n_estimators = 100 )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 47
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print \"Fitting a random forest to labeled training data...\"\n",
      "forest = forest.fit( trainDataVecs, train[\"sentiment\"] )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Fitting a random forest to labeled training data...\n"
       ]
      }
     ],
     "prompt_number": 48
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "result = forest.predict( testDataVecs )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 49
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "output = pd.DataFrame( data={\"id\":test[\"id\"], \"sentiment\":result} )\n",
      "output.to_csv( \"Word2Vec_AverageVectors.csv\", index=False, quoting=3 )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 50
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}